<!-- Copyright (c) 2020 ARM Limited. -->
<!--                                 -->
<!-- SPDX-License-Identifier: MIT    -->
<!--                                 -->
<!-- HTML header for doxygen 1.8.13-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="robots" content="NOINDEX, NOFOLLOW" />
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ArmNN: Arm NN</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="stylesheet.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <img alt="ArmNN" src="Arm_NN_horizontal_blue.png" style="max-width: 10rem; margin-top: .5rem; margin-left 10px"/>
  <td style="padding-left: 0.5em;">
   <div id="projectname">
   &#160;<span id="projectnumber">22.05.01</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('index.xhtml','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Arm NN </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><ul>
<li><a href="#quick-start-guides">Quick Start Guides</a></li>
<li><a href="#pre-built-binaries">Pre-Built Binaries</a></li>
<li><a href="#software-overview">Software Overview</a></li>
<li><a href="#get-involved">Get Involved</a></li>
<li><a href="#contributions">Contributions</a></li>
<li><a href="#disclaimer">Disclaimer</a></li>
<li><a href="#license">License</a></li>
<li><a href="#third-party">Third-Party</a></li>
<li><a href="#build-flags">Build Flags</a></li>
</ul>
<p>**_Arm NN_** is the <b>most performant</b> machine learning (ML) inference engine for Android and Linux, accelerating ML on <b>Arm Cortex-A CPUs and Arm Mali GPUs</b>. This ML inference engine is an open source SDK which bridges the gap between existing neural network frameworks and power-efficient Arm IP.</p>
<p>Arm NN outperforms generic ML libraries due to <b>Arm architecture-specific optimizations</b> (e.g. SVE2) by utilizing <b><a href="https://github.com/ARM-software/ComputeLibrary/">Arm Compute Library (ACL)</a></b>. To target Arm Ethos-N NPUs, Arm NN utilizes the <a href="https://github.com/ARM-software/ethos-n-driver-stack">Ethos-N NPU Driver</a>. For Arm Cortex-M acceleration, please see <a href="https://github.com/ARM-software/CMSIS_5">CMSIS-NN</a>.</p>
<p>Arm NN is written using portable <b>C++14</b> and built using <a href="https://cmake.org/">CMake</a> - enabling builds for a wide variety of target platforms, from a wide variety of host environments. <b>Python</b> developers can interface with Arm NN through the use of our <b>Arm NN TF Lite Delegate</b>.</p>
<h2>Quick Start Guides</h2>
<p><b>The Arm NN TF Lite Delegate provides the widest ML operator support in Arm NN</b> and is an easy way to accelerate your ML model. To start using the TF Lite Delegate, first download the <b><a href="#pre-built-binaries">Pre-Built Binaries</a></b> for the latest release of Arm NN. Using a Python interpreter, you can load your TF Lite model into the Arm NN TF Lite Delegate and run accelerated inference. Please see this <b><a class="el" href="md_delegate__delegate_quick_start_guide.xhtml">Quick Start Guide</a></b> on GitHub or this more comprehensive <b><a href="https://developer.arm.com/documentation/102561/latest/">Arm Developer Guide</a></b> for information on how to accelerate your TF Lite model using the Arm NN TF Lite Delegate.</p>
<p>The fastest way to integrate Arm NN into an <b>Android app</b> is by using our <b>Arm NN AAR (Android Archive) file with Android Studio</b>. The AAR file nicely packages up the Arm NN TF Lite Delegate, Arm NN itself and ACL; ready to be integrated into your Android ML application. Using the AAR allows you to benefit from the <b>vast operator support</b> of the Arm NN TF Lite Delegate. We held an <b><a href="https://www.youtube.com/watch?v=Zu4v0nqq2FA">Arm AI Tech Talk</a></b> on how to accelerate an ML Image Segmentation app in 5 minutes using this AAR file, with the supporting guide <b><a href="https://developer.arm.com/documentation/102744/latest">here</a></b>. To download the Arm NN AAR file, please see the <b><a href="#pre-built-binaries">Pre-Built Binaries</a></b> section below.</p>
<p>We also provide Debian packages for Arm NN, which are a quick way to start using Arm NN and the TF Lite Parser (albeit with less ML operator support than the TF Lite Delegate). There is an installation guide available <a class="el" href="md__installation_via_apt_repository.xhtml">here</a> which provides instructions on how to install the Arm NN Core and the TF Lite Parser for Ubuntu 20.04.</p>
<h2>Pre-Built Binaries</h2>
<table class="doxtable">
<tr>
<th>Operating System </th><th>Architecture-specific Release Archive (Download)  </th></tr>
<tr>
<td>Android (AAR) </td><td><a href="https://github.com/ARM-software/armnn/releases/download/v22.02/ArmnnDelegate-release.aar"></a> </td></tr>
<tr>
<td>Android 27 </td><td><a href="https://github.com/ARM-software/armnn/releases/download/v22.02/ArmNN-android-27-arm64-v8.2-a.tar.gz"></a> <a href="https://github.com/ARM-software/armnn/releases/download/v22.02/ArmNN-android-27-arm64-v8a.tar.gz"></a> <a href="https://github.com/ARM-software/armnn/releases/download/v22.02/ArmNN-android-27-armv7a.tar.gz"></a> </td></tr>
<tr>
<td>Android 28 </td><td><a href="https://github.com/ARM-software/armnn/releases/download/v22.02/ArmNN-android-28-arm64-v8.2-a.tar.gz"></a> <a href="https://github.com/ARM-software/armnn/releases/download/v22.02/ArmNN-android-28-arm64-v8a.tar.gz"></a> <a href="https://github.com/ARM-software/armnn/releases/download/v22.02/ArmNN-android-28-armv7a.tar.gz"></a> </td></tr>
<tr>
<td>Android 29 </td><td><a href="https://github.com/ARM-software/armnn/releases/download/v22.02/ArmNN-android-29-arm64-v8.2-a.tar.gz"></a> <a href="https://github.com/ARM-software/armnn/releases/download/v22.02/ArmNN-android-29-arm64-v8a.tar.gz"></a> <a href="https://github.com/ARM-software/armnn/releases/download/v22.02/ArmNN-android-29-armv7a.tar.gz"></a> </td></tr>
<tr>
<td>Linux </td><td><a href="https://github.com/ARM-software/armnn/releases/download/v22.02/ArmNN-linux-aarch64.tar.gz"></a> <a href="https://github.com/ARM-software/armnn/releases/download/v22.02/ArmNN-linux-x86_64.tar.gz"></a> </td></tr>
</table>
<h2>Software Overview</h2>
<p>The Arm NN SDK supports ML models in <b>TensorFlow Lite</b> (TF Lite) and <b>ONNX</b> formats.</p>
<p><b>Arm NN's TF Lite Delegate</b> accelerates TF Lite models through <b>Python or C++ APIs</b>. Supported TF Lite operators are accelerated by Arm NN and any unsupported operators are delegated (fallback) to the reference TF Lite runtime - ensuring extensive ML operator support. <b>The recommended way to use Arm NN is to <a href="https://www.tensorflow.org/lite/convert">convert your model to TF Lite format</a> and use the TF Lite Delegate.</b> Please refer to the <a href="#quick-start-guides">Quick Start Guides</a> for more information on how to use the TF Lite Delegate.</p>
<p>Arm NN also provides <b>TF Lite and ONNX parsers</b> which are C++ libraries for integrating TF Lite or ONNX models into your ML application. Please note that these parsers do not provide extensive ML operator coverage as compared to the Arm NN TF Lite Delegate.</p>
<p><b>Android</b> ML application developers have a number of options for using Arm NN:</p><ul>
<li>Use our Arm NN AAR (Android Archive) file with <b>Android Studio</b> as described in the <a href="#quick-start-guides">Quick Start Guides</a> section</li>
<li>Download and use our <a href="#pre-built-binaries">Pre-Built Binaries</a> for the Android platform</li>
<li>Build Arm NN from scratch with the Android NDK using this <a class="el" href="md__build_guide_android_n_d_k.xhtml">GitHub guide</a></li>
</ul>
<p>Arm also provides an <a href="https://github.com/ARM-software/android-nn-driver">Android-NN-Driver</a> which implements a hardware abstraction layer (HAL) for the Android NNAPI. When the Android NN Driver is integrated on an Android device, ML models used in Android applications will automatically be accelerated by Arm NN.</p>
<p>For more information about the Arm NN components, please refer to our <a href="https://github.com/ARM-software/armnn/wiki/Documentation">documentation</a>.</p>
<p>Arm NN is a key component of the <a href="https://mlplatform.org/">machine learning platform</a>, which is part of the <a href="https://www.linaro.org/news/linaro-announces-launch-of-machine-intelligence-initiative/">Linaro Machine Intelligence Initiative</a>.</p>
<p>For FAQs and troubleshooting advice, see the <a class="el" href="md_docs__f_a_q.xhtml">FAQ</a> or take a look at previous <a href="https://github.com/ARM-software/armnn/issues">GitHub Issues</a>.</p>
<h2>Get Involved</h2>
<p>The best way to get involved is by using our software. If you need help or encounter and issue, please raise it as a <a href="https://github.com/ARM-software/armnn/issues">GitHub Issue</a>. Feel free to have a look at any of our open issues too. We also welcome feedback on our documentation.</p>
<p>Feature requests without a volunteer to implement them are closed, but have the 'Help wanted' label, these can be found <a href="https://github.com/ARM-software/armnn/issues?q=is%3Aissue+label%3A%22Help+wanted%22+">here</a>. Once you find a suitable Issue, feel free to re-open it and add a comment, so that Arm NN engineers know you are working on it and can help.</p>
<p>When the feature is implemented the 'Help wanted' label will be removed.</p>
<h2>Contributions</h2>
<p>The Arm NN project welcomes contributions. For more details on contributing to Arm NN please see the <a href="https://mlplatform.org/contributing/">Contributing page</a> on the <a href="https://mlplatform.org/">MLPlatform.org</a> website, or see the <a class="el" href="md__contributor_guide.xhtml">Contributor Guide</a>.</p>
<p>Particularly if you'd like to implement your own backend next to our CPU, GPU and NPU backends there are guides for backend development: <a class="el" href="md_src_backends__r_e_a_d_m_e.xhtml">Backend development guide</a>, <a class="el" href="md_src_dynamic__r_e_a_d_m_e.xhtml">Dynamic backend development guide</a>.</p>
<h2>Disclaimer</h2>
<p>The armnn/tests directory contains tests used during Arm NN development. Many of them depend on third-party IP, model protobufs and image files not distributed with Arm NN. The dependencies for some tests are available freely on the Internet, for those who wish to experiment, but they won't run out of the box.</p>
<h2>License</h2>
<p>Arm NN is provided under the <a href="https://spdx.org/licenses/MIT.html">MIT</a> license. See [LICENSE](LICENSE) for more information. Contributions to this project are accepted under the same license.</p>
<p>Individual files contain the following tag instead of the full license text. </p><pre class="fragment">SPDX-License-Identifier: MIT
</pre><p>This enables machine processing of license information based on the SPDX License Identifiers that are available here: <a href="http://spdx.org/licenses/">http://spdx.org/licenses/</a></p>
<h2>Third-party</h2>
<p>Third party tools used by Arm NN:</p>
<table class="doxtable">
<tr>
<th>Tool </th><th>License (SPDX ID) </th><th>Description </th><th>Version </th><th>Provenience  </th></tr>
<tr>
<td>cxxopts </td><td>MIT </td><td>A lightweight C++ option parser library </td><td>SHA 12e496da3d486b87fa9df43edea65232ed852510 </td><td><a href="https://github.com/jarro2783/cxxopts">https://github.com/jarro2783/cxxopts</a> </td></tr>
<tr>
<td>doctest </td><td>MIT </td><td>Header-only C++ testing framework </td><td>2.4.6 </td><td><a href="https://github.com/onqtam/doctest">https://github.com/onqtam/doctest</a> </td></tr>
<tr>
<td>fmt </td><td>MIT </td><td>{fmt} is an open-source formatting library providing a fast and safe alternative to C stdio and C++ iostreams. </td><td>7.0.1 </td><td><a href="https://github.com/fmtlib/fmt">https://github.com/fmtlib/fmt</a> </td></tr>
<tr>
<td>ghc </td><td>MIT </td><td>A header-only single-file std::filesystem compatible helper library </td><td>1.3.2 </td><td><a href="https://github.com/gulrak/filesystem">https://github.com/gulrak/filesystem</a> </td></tr>
<tr>
<td>half </td><td>MIT </td><td>IEEE 754 conformant 16-bit half-precision floating point library </td><td>1.12.0 </td><td><a href="http://half.sourceforge.net">http://half.sourceforge.net</a> </td></tr>
<tr>
<td>mapbox/variant </td><td>BSD </td><td>A header-only alternative to 'boost::variant' </td><td>1.1.3 </td><td><a href="https://github.com/mapbox/variant">https://github.com/mapbox/variant</a> </td></tr>
<tr>
<td>stb </td><td>MIT </td><td>Image loader, resize and writer </td><td>2.16 </td><td><a href="https://github.com/nothings/stb">https://github.com/nothings/stb</a> </td></tr>
</table>
<h2>Build Flags</h2>
<p>Arm NN uses the following security related build flags in their code:</p>
<table class="doxtable">
<tr>
<th>Build flags  </th></tr>
<tr>
<td>-Wall </td></tr>
<tr>
<td>-Wextra </td></tr>
<tr>
<td>-Wold-style-cast </td></tr>
<tr>
<td>-Wno-missing-braces </td></tr>
<tr>
<td>-Wconversion </td></tr>
<tr>
<td>-Wsign-conversion </td></tr>
<tr>
<td>-Werror </td></tr>
</table>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Fri Jun 17 2022 13:20:38 for ArmNN by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
